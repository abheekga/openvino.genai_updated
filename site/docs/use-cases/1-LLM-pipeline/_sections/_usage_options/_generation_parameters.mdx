### Use Different Generation Parameters

Fine-tune your LLM's output by adjusting various generation parameters.
OpenVINO GenAI supports multiple sampling strategies and generation configurations to help you achieve the desired balance between deterministic and creative outputs.

#### Basic Generation Configuration

1. Get the model default config with `get_generation_config()`
2. Modify parameters
3. Apply the updated config:
    - Use `set_generation_config(config)`
    - Pass config directly to `generate()` (e.g. `generate(prompt, config)`)
    - Specify options as inputs in the `generate()` method (e.g. `generate(prompt, max_new_tokens=100)`)

<LanguageTabs>
    <TabItemPython>
        ```python
        import openvino_genai as ov_genai
        pipe = ov_genai.LLMPipeline(model_path, "CPU")

        # Get default configuration
        config = pipe.get_generation_config()

        # Modify parameters
        config.max_new_tokens = 100
        config.temperature = 0.7
        config.top_k = 50
        config.top_p = 0.9
        config.repetition_penalty = 1.2

        # Generate text with custom configuration
        output = pipe.generate("The Sun is yellow because", config)
        ```
    </TabItemPython>
    <TabItemCpp>
        ```cpp
        int main() {
            ov::genai::LLMPipeline pipe(model_path, "CPU");

            // Get default configuration
            auto config = pipe.get_generation_config();

            // Modify parameters
            config.max_new_tokens = 100;
            config.temperature = 0.7f;
            config.top_k = 50;
            config.top_p = 0.9f;
            config.repetition_penalty = 1.2f;

            // Generate text with custom configuration
            auto output = pipe.generate("The Sun is yellow because", config);
        }
        ```
    </TabItemCpp>
</LanguageTabs>

:::info Understanding Basic Generation Parameters

- `max_new_tokens`: The maximum numbers of tokens to generate, excluding the number of tokens in the prompt. `max_new_tokens` has priority over `max_length`.
- `temperature`: Controls the level of creativity in AI-generated text:
    - Low temperature (e.g. 0.2) leads to more focused and deterministic output, choosing tokens with the highest probability.
    - Medium temperature (e.g. 1.0) maintains a balance between creativity and focus, selecting tokens based on their probabilities without significant bias.
    - High temperature (e.g. 2.0) makes output more creative and adventurous, increasing the chances of selecting less likely tokens.
- `top_k`: Limits token selection to the k most likely next tokens. Higher values allow more diverse outputs.
- `top_p`: Selects from the smallest set of tokens whose cumulative probability exceeds p. Helps balance diversity and quality.
- `repetition_penalty`: Reduces the likelihood of repeating tokens. Values above 1.0 discourage repetition.

For the full list of generation parameters, refer to the [API reference](https://docs.openvino.ai/2025/api/genai_api/_autosummary/openvino_genai.GenerationConfig.html#openvino-genai-generationconfig).

:::


#### Optimizing Generation with Grouped Beam Search

Beam search helps explore multiple possible text completions simultaneously, often leading to higher quality outputs.

<LanguageTabs>
    <TabItemPython>
        ```python
        import openvino_genai as ov_genai
        pipe = ov_genai.LLMPipeline(model_path, "CPU")

        # Get default generation config
        config = pipe.get_generation_config()

        # Modify parameters
        config.max_new_tokens = 256
        config.num_beams = 15
        config.num_beam_groups = 3
        config.diversity_penalty = 1.0

        # Generate text with custom configuration
        print(pipe.generate("The Sun is yellow because", config))
        ```
    </TabItemPython>
    <TabItemCpp>
        ```cpp
        int main(int argc, char* argv[]) {
            std::string model_path = argv[1];
            ov::genai::LLMPipeline pipe(model_path, "CPU");

            // Get default generation config
            ov::genai::GenerationConfig config = pipe.get_generation_config();

            // Modify parameters
            config.max_new_tokens = 256;
            config.num_beams = 15;
            config.num_beam_groups = 3;
            config.diversity_penalty = 1.0f;

            // Generate text with custom configuration
            cout << pipe.generate("The Sun is yellow because", config);
        }
        ```
    </TabItemCpp>
</LanguageTabs>

:::info Understanding Beam Search Generation Parameters

- `max_new_tokens`: The maximum numbers of tokens to generate, excluding the number of tokens in the prompt. `max_new_tokens` has priority over `max_length`.
- `num_beams`: The number of beams for beam search. 1 disables beam search.
- `num_beam_groups`: The number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.
- `diversity_penalty`: value is subtracted from a beam's score if it generates the same token as any beam from other group at a particular time.

For the full list of generation parameters, refer to the [API reference](https://docs.openvino.ai/2025/api/genai_api/_autosummary/openvino_genai.GenerationConfig.html#openvino-genai-generationconfig).

:::
